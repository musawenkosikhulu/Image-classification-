# -*- coding: utf-8 -*-
"""musawenkosi_Image_Classification_Practical5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aOFozO6Q8ngGqFX0i8W-EawbWCeefuPP

## Image classification Practical

***NOTE***

Be sure to use hardware acceleration to use the GPU. Click on `Runtime`, change `runtime type`, and select `GPU` for the *hardware accelerator* option.

<hr>

Total marks: 33

Familiarise yourself with the data set: https://www.cs.toronto.edu/~kriz/cifar.html

Question 1: how many classes are in cifar-10? **answer**: ***10 classes***

Question 2: is this a colour or greyscale image dataset? **answer**: ***colour image set.***

Question 3: how many channels/depth should the images have? **answer**: ***3 channels.***

Question 4: is this a regression or classification problem? **answer**: ***classification problem***

Question 5: if we one-hot encode the targets for this dataset, what will the dimensions of the resulting vector be? **answer**: ***10***

Question 6: which activation function will you most likely use in the last layer of your model for this problem? **answer**: ***softmax***
"""

from keras.datasets import cifar10

# Commented out IPython magic to ensure Python compatibility.
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, MaxPool3D, Flatten, Dropout
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.callbacks import ModelCheckpoint
import numpy as np
import matplotlib.pyplot as plt
from keras.utils import np_utils
from sklearn.metrics import accuracy_score, confusion_matrix
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder
import pandas as pd
from sklearn.model_selection import train_test_split
# %matplotlib inline

"""## Load the dataset"""

(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()

"""#Task: do all preprocessing here

The data is obtained as a training and testing set. Typically, it's a good idea to create a validation set from the training set. Hint: previous pracs illustrated how to do this. Name the validation data as `X_val` and `Y_val`.

All other preprocessing including normalisation, one-hot encoding etc should be added here...
"""

X_train[0]

Y_train[0]

"""### Check the maximum to normalation the data """

np.max(X_train)

np.min(X_train)

"""Normalisation """

X_train = X_train/255
X_test = X_test/255

"""### Take a look at a single image (after normalisation)"""

X_train[0]

"""### What is the min/max for this image after normalisation?"""

np.max(X_train[0])

np.min(X_train[0])

"""### One hot encoding

### Before
"""

Y_test[0]

"""One-hot encoding"""

Y_train = np_utils.to_categorical(Y_train)
Y_test = np_utils.to_categorical(Y_test)

Y_train[0]

"""### Validation Data"""

X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.3, random_state=1632)

print('Training data shape : ', X_train.shape, Y_train.shape)

print('Testing data shape : ', X_test.shape, Y_test.shape)

"""## Create a convolutional neural network

Your network must get at least 70% accuracy on the test set.

Your model must have less than 600,000 weights in total.

Hint: try first with no dropout, then add dropout - what happens, is it bettter or worse?

Hint: try small batch size
"""

def baseline_model():
    # create model
    model = Sequential()
    
    # Since this is the first layer, we need to specify the input shape. Only here, only once.
    # We are creating 64 filters each of size 2x2. What will be the depth of each of those 64 filters?
    # What will be the resulting depth of the feature map after applying these filters?

    # "valid" means no padding. "same" results in padding with zeros evenly to the 
    # left/right or up/down of the input such that output has the same height/width dimension as the input. 
    model.add(Conv2D(filters=256, kernel_size=2, activation='relu', input_shape=(32,32,3))) 
    
    # Here we create a 2x2 max pooling layer
    model.add(MaxPool2D(pool_size=4))

    model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu')) 
    model.add(MaxPool2D(pool_size=6))

    # In order to pass output from the convolutional block to the dense block, we must flatten each example in the minibatch. 
    # In other words, we take this four-dimensional input [batch, width, height, depth] and transform it into the two-dimensional input [batch, units/input dimensions] expected by fully-connected layers
    model.add(Flatten())
    
    model.add(Dense(256, activation='relu'))

    ##model.add(Dropout(0.5))
    
    model.add(Dense(10, activation='softmax'))

    loss = CategoricalCrossentropy()
    
    # Compile the model
    model.compile(loss=loss,
             optimizer='adam',
             metrics=['accuracy'])
    return model

model = baseline_model()

"""## Determine the number of trainable parameters"""

model.summary()# Your code here

"""## Train the model

"""

history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=20, batch_size=84)

"""## Plot the performance

Plot the training and validation accuracy in a single plot
"""

def plot_hist(h, xsize=6, ysize=10):

    fig_size = plt.rcParams["figure.figsize"]
    plt.rcParams["figure.figsize"] = [xsize, ysize]
    fig, axes = plt.subplots(nrows=1, ncols=1, sharex=True)
    
    # summarize history for Accuracy
    plt.subplot(211)
    plt.plot(h['accuracy'])
    plt.plot(h['val_accuracy'])
    plt.title('Training Performance')
    plt.ylabel('Accuracy')
    plt.xlabel('Epochs')
    plt.legend(['Train', 'Validation'], loc='best')
    
    plt.draw()
    plt.show()

    return

plot_hist(history.history, xsize=8, ysize=12)

"""## Predict on the test data"""

predictions = model.predict(X_test)
predictions = np.argmax(predictions, axis=-1)

"""## Compute the accuracy

Get at least 65% testing accuracy
"""

correct_values = np.argmax(Y_test,axis=1)


accuracy_score(predictions,correct_values)*100

"""## Obtain the confusion matrix"""

confusion_matrix(correct_values, predictions)